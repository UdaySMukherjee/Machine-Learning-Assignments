{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO7fiMgwrkv1+WvG54Jbgj8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UdaySMukherjee/Machine-Learning-Tasks/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#hyperparameter\n",
        "num_epochs = 50\n",
        "batch_size = 4\n",
        "learning_rate = 0.001\n",
        "\n",
        "#dataset has PILImages images of range [0,1]\n",
        "#We transform them to Tensors of normalized range [-1,1]\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data',train=True,download=True,transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,shuffle=False)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data',train=True,download=True,transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,shuffle=False)\n",
        "\n",
        "classes = ('plane','car','bird','cat','deer','dog','frog','horse',\"ship\",'truck')\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = ConvNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i,(images,labels) in enumerate(train_loader):\n",
        "        #origin shape: [4,3,32,32] = 4,3,1024\n",
        "        #input_layer: 3 input channels ,6 output channels, 5 kernel size\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        #forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs,labels)\n",
        "\n",
        "        #backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if(i+1)%2000==0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}],Loss: {loss.item():.4f}')\n",
        "\n",
        "print('Training Finished')\n",
        "\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            label = labels[i]\n",
        "            pred = predicted[i]\n",
        "            if label == pred:\n",
        "                n_class_correct[label] += 1\n",
        "            n_class_samples[label] += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc:.2f}%')\n",
        "\n",
        "    for i in range(10):\n",
        "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of {classes[i]}: {acc:.2f}%')\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrdigAXCpZTO",
        "outputId": "51cf23eb-d287-4445-baeb-857a3850f3b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/50], Step [2000/12500],Loss: 2.3518\n",
            "Epoch [1/50], Step [4000/12500],Loss: 2.2950\n",
            "Epoch [1/50], Step [6000/12500],Loss: 2.3295\n",
            "Epoch [1/50], Step [8000/12500],Loss: 2.2915\n",
            "Epoch [1/50], Step [10000/12500],Loss: 2.2996\n",
            "Epoch [1/50], Step [12000/12500],Loss: 2.3136\n",
            "Epoch [2/50], Step [2000/12500],Loss: 2.2429\n",
            "Epoch [2/50], Step [4000/12500],Loss: 2.1819\n",
            "Epoch [2/50], Step [6000/12500],Loss: 1.6538\n",
            "Epoch [2/50], Step [8000/12500],Loss: 1.8073\n",
            "Epoch [2/50], Step [10000/12500],Loss: 2.5207\n",
            "Epoch [2/50], Step [12000/12500],Loss: 1.5578\n",
            "Epoch [3/50], Step [2000/12500],Loss: 2.4744\n",
            "Epoch [3/50], Step [4000/12500],Loss: 1.9921\n",
            "Epoch [3/50], Step [6000/12500],Loss: 1.7586\n",
            "Epoch [3/50], Step [8000/12500],Loss: 1.6442\n",
            "Epoch [3/50], Step [10000/12500],Loss: 2.6428\n",
            "Epoch [3/50], Step [12000/12500],Loss: 1.3189\n",
            "Epoch [4/50], Step [2000/12500],Loss: 2.3051\n",
            "Epoch [4/50], Step [4000/12500],Loss: 1.9981\n",
            "Epoch [4/50], Step [6000/12500],Loss: 1.4387\n",
            "Epoch [4/50], Step [8000/12500],Loss: 1.4391\n",
            "Epoch [4/50], Step [10000/12500],Loss: 2.5434\n",
            "Epoch [4/50], Step [12000/12500],Loss: 1.4232\n",
            "Epoch [5/50], Step [2000/12500],Loss: 2.2738\n",
            "Epoch [5/50], Step [4000/12500],Loss: 2.2192\n",
            "Epoch [5/50], Step [6000/12500],Loss: 1.3101\n",
            "Epoch [5/50], Step [8000/12500],Loss: 1.3446\n",
            "Epoch [5/50], Step [10000/12500],Loss: 2.3603\n",
            "Epoch [5/50], Step [12000/12500],Loss: 1.3338\n",
            "Epoch [6/50], Step [2000/12500],Loss: 2.1840\n",
            "Epoch [6/50], Step [4000/12500],Loss: 2.3485\n",
            "Epoch [6/50], Step [6000/12500],Loss: 1.2857\n",
            "Epoch [6/50], Step [8000/12500],Loss: 1.3322\n",
            "Epoch [6/50], Step [10000/12500],Loss: 2.1843\n",
            "Epoch [6/50], Step [12000/12500],Loss: 1.1671\n",
            "Epoch [7/50], Step [2000/12500],Loss: 2.0166\n",
            "Epoch [7/50], Step [4000/12500],Loss: 2.3208\n",
            "Epoch [7/50], Step [6000/12500],Loss: 1.3415\n",
            "Epoch [7/50], Step [8000/12500],Loss: 1.2980\n",
            "Epoch [7/50], Step [10000/12500],Loss: 2.0279\n",
            "Epoch [7/50], Step [12000/12500],Loss: 1.0688\n",
            "Epoch [8/50], Step [2000/12500],Loss: 1.9282\n",
            "Epoch [8/50], Step [4000/12500],Loss: 2.2431\n",
            "Epoch [8/50], Step [6000/12500],Loss: 1.2864\n",
            "Epoch [8/50], Step [8000/12500],Loss: 1.2460\n",
            "Epoch [8/50], Step [10000/12500],Loss: 1.9164\n",
            "Epoch [8/50], Step [12000/12500],Loss: 0.9636\n",
            "Epoch [9/50], Step [2000/12500],Loss: 1.8451\n",
            "Epoch [9/50], Step [4000/12500],Loss: 2.2370\n",
            "Epoch [9/50], Step [6000/12500],Loss: 1.2346\n",
            "Epoch [9/50], Step [8000/12500],Loss: 1.2237\n",
            "Epoch [9/50], Step [10000/12500],Loss: 1.8293\n",
            "Epoch [9/50], Step [12000/12500],Loss: 0.8888\n",
            "Epoch [10/50], Step [2000/12500],Loss: 1.8397\n",
            "Epoch [10/50], Step [4000/12500],Loss: 2.1883\n",
            "Epoch [10/50], Step [6000/12500],Loss: 1.2215\n",
            "Epoch [10/50], Step [8000/12500],Loss: 1.2155\n",
            "Epoch [10/50], Step [10000/12500],Loss: 1.6923\n",
            "Epoch [10/50], Step [12000/12500],Loss: 0.8416\n",
            "Epoch [11/50], Step [2000/12500],Loss: 1.8303\n",
            "Epoch [11/50], Step [4000/12500],Loss: 2.0521\n",
            "Epoch [11/50], Step [6000/12500],Loss: 1.1751\n",
            "Epoch [11/50], Step [8000/12500],Loss: 1.2303\n",
            "Epoch [11/50], Step [10000/12500],Loss: 1.5663\n",
            "Epoch [11/50], Step [12000/12500],Loss: 0.8311\n",
            "Epoch [12/50], Step [2000/12500],Loss: 1.7344\n",
            "Epoch [12/50], Step [4000/12500],Loss: 1.9290\n",
            "Epoch [12/50], Step [6000/12500],Loss: 1.1652\n",
            "Epoch [12/50], Step [8000/12500],Loss: 1.2495\n",
            "Epoch [12/50], Step [10000/12500],Loss: 1.4846\n",
            "Epoch [12/50], Step [12000/12500],Loss: 0.7588\n",
            "Epoch [13/50], Step [2000/12500],Loss: 1.6861\n",
            "Epoch [13/50], Step [4000/12500],Loss: 1.9000\n",
            "Epoch [13/50], Step [6000/12500],Loss: 1.1713\n",
            "Epoch [13/50], Step [8000/12500],Loss: 1.2423\n",
            "Epoch [13/50], Step [10000/12500],Loss: 1.3488\n",
            "Epoch [13/50], Step [12000/12500],Loss: 0.7434\n",
            "Epoch [14/50], Step [2000/12500],Loss: 1.5617\n",
            "Epoch [14/50], Step [4000/12500],Loss: 1.8083\n",
            "Epoch [14/50], Step [6000/12500],Loss: 1.1693\n",
            "Epoch [14/50], Step [8000/12500],Loss: 1.1673\n",
            "Epoch [14/50], Step [10000/12500],Loss: 1.2456\n",
            "Epoch [14/50], Step [12000/12500],Loss: 0.7638\n",
            "Epoch [15/50], Step [2000/12500],Loss: 1.4412\n",
            "Epoch [15/50], Step [4000/12500],Loss: 1.7829\n",
            "Epoch [15/50], Step [6000/12500],Loss: 1.1389\n",
            "Epoch [15/50], Step [8000/12500],Loss: 0.9878\n",
            "Epoch [15/50], Step [10000/12500],Loss: 1.1895\n",
            "Epoch [15/50], Step [12000/12500],Loss: 0.7452\n",
            "Epoch [16/50], Step [2000/12500],Loss: 1.4010\n",
            "Epoch [16/50], Step [4000/12500],Loss: 1.7886\n",
            "Epoch [16/50], Step [6000/12500],Loss: 1.1268\n",
            "Epoch [16/50], Step [8000/12500],Loss: 0.9285\n",
            "Epoch [16/50], Step [10000/12500],Loss: 1.0941\n",
            "Epoch [16/50], Step [12000/12500],Loss: 0.7208\n",
            "Epoch [17/50], Step [2000/12500],Loss: 1.3390\n",
            "Epoch [17/50], Step [4000/12500],Loss: 1.6900\n",
            "Epoch [17/50], Step [6000/12500],Loss: 1.1521\n",
            "Epoch [17/50], Step [8000/12500],Loss: 0.8868\n",
            "Epoch [17/50], Step [10000/12500],Loss: 1.0212\n",
            "Epoch [17/50], Step [12000/12500],Loss: 0.7521\n",
            "Epoch [18/50], Step [2000/12500],Loss: 1.2888\n",
            "Epoch [18/50], Step [4000/12500],Loss: 1.6577\n",
            "Epoch [18/50], Step [6000/12500],Loss: 1.1365\n",
            "Epoch [18/50], Step [8000/12500],Loss: 0.8672\n",
            "Epoch [18/50], Step [10000/12500],Loss: 0.9592\n",
            "Epoch [18/50], Step [12000/12500],Loss: 0.7498\n",
            "Epoch [19/50], Step [2000/12500],Loss: 1.2784\n",
            "Epoch [19/50], Step [4000/12500],Loss: 1.6104\n",
            "Epoch [19/50], Step [6000/12500],Loss: 1.1323\n",
            "Epoch [19/50], Step [8000/12500],Loss: 0.8285\n",
            "Epoch [19/50], Step [10000/12500],Loss: 0.9181\n",
            "Epoch [19/50], Step [12000/12500],Loss: 0.7469\n",
            "Epoch [20/50], Step [2000/12500],Loss: 1.2161\n",
            "Epoch [20/50], Step [4000/12500],Loss: 1.4833\n",
            "Epoch [20/50], Step [6000/12500],Loss: 1.0986\n",
            "Epoch [20/50], Step [8000/12500],Loss: 0.8444\n",
            "Epoch [20/50], Step [10000/12500],Loss: 0.8154\n",
            "Epoch [20/50], Step [12000/12500],Loss: 0.7445\n",
            "Epoch [21/50], Step [2000/12500],Loss: 1.1753\n",
            "Epoch [21/50], Step [4000/12500],Loss: 1.4515\n",
            "Epoch [21/50], Step [6000/12500],Loss: 1.1543\n",
            "Epoch [21/50], Step [8000/12500],Loss: 0.9001\n",
            "Epoch [21/50], Step [10000/12500],Loss: 0.7379\n",
            "Epoch [21/50], Step [12000/12500],Loss: 0.7204\n",
            "Epoch [22/50], Step [2000/12500],Loss: 1.1216\n",
            "Epoch [22/50], Step [4000/12500],Loss: 1.3507\n",
            "Epoch [22/50], Step [6000/12500],Loss: 1.1147\n",
            "Epoch [22/50], Step [8000/12500],Loss: 0.8523\n",
            "Epoch [22/50], Step [10000/12500],Loss: 0.7260\n",
            "Epoch [22/50], Step [12000/12500],Loss: 0.7216\n",
            "Epoch [23/50], Step [2000/12500],Loss: 1.0139\n",
            "Epoch [23/50], Step [4000/12500],Loss: 1.3159\n",
            "Epoch [23/50], Step [6000/12500],Loss: 1.1447\n",
            "Epoch [23/50], Step [8000/12500],Loss: 0.7866\n",
            "Epoch [23/50], Step [10000/12500],Loss: 0.6291\n",
            "Epoch [23/50], Step [12000/12500],Loss: 0.6726\n",
            "Epoch [24/50], Step [2000/12500],Loss: 0.9863\n",
            "Epoch [24/50], Step [4000/12500],Loss: 1.2660\n",
            "Epoch [24/50], Step [6000/12500],Loss: 1.1649\n",
            "Epoch [24/50], Step [8000/12500],Loss: 0.8005\n",
            "Epoch [24/50], Step [10000/12500],Loss: 0.5641\n",
            "Epoch [24/50], Step [12000/12500],Loss: 0.6356\n",
            "Epoch [25/50], Step [2000/12500],Loss: 0.9784\n",
            "Epoch [25/50], Step [4000/12500],Loss: 1.2788\n",
            "Epoch [25/50], Step [6000/12500],Loss: 1.0963\n",
            "Epoch [25/50], Step [8000/12500],Loss: 0.7850\n",
            "Epoch [25/50], Step [10000/12500],Loss: 0.4769\n",
            "Epoch [25/50], Step [12000/12500],Loss: 0.5625\n",
            "Epoch [26/50], Step [2000/12500],Loss: 0.9446\n",
            "Epoch [26/50], Step [4000/12500],Loss: 1.2879\n",
            "Epoch [26/50], Step [6000/12500],Loss: 1.0921\n",
            "Epoch [26/50], Step [8000/12500],Loss: 0.7235\n",
            "Epoch [26/50], Step [10000/12500],Loss: 0.4368\n",
            "Epoch [26/50], Step [12000/12500],Loss: 0.5051\n",
            "Epoch [27/50], Step [2000/12500],Loss: 0.8899\n",
            "Epoch [27/50], Step [4000/12500],Loss: 1.2306\n",
            "Epoch [27/50], Step [6000/12500],Loss: 1.0903\n",
            "Epoch [27/50], Step [8000/12500],Loss: 0.7219\n",
            "Epoch [27/50], Step [10000/12500],Loss: 0.3738\n",
            "Epoch [27/50], Step [12000/12500],Loss: 0.4637\n",
            "Epoch [28/50], Step [2000/12500],Loss: 0.8824\n",
            "Epoch [28/50], Step [4000/12500],Loss: 1.2263\n",
            "Epoch [28/50], Step [6000/12500],Loss: 1.0342\n",
            "Epoch [28/50], Step [8000/12500],Loss: 0.7099\n",
            "Epoch [28/50], Step [10000/12500],Loss: 0.3441\n",
            "Epoch [28/50], Step [12000/12500],Loss: 0.4762\n",
            "Epoch [29/50], Step [2000/12500],Loss: 0.9220\n",
            "Epoch [29/50], Step [4000/12500],Loss: 1.2382\n",
            "Epoch [29/50], Step [6000/12500],Loss: 0.9865\n",
            "Epoch [29/50], Step [8000/12500],Loss: 0.7160\n",
            "Epoch [29/50], Step [10000/12500],Loss: 0.3069\n",
            "Epoch [29/50], Step [12000/12500],Loss: 0.4135\n",
            "Epoch [30/50], Step [2000/12500],Loss: 0.8357\n",
            "Epoch [30/50], Step [4000/12500],Loss: 1.2042\n",
            "Epoch [30/50], Step [6000/12500],Loss: 1.0155\n",
            "Epoch [30/50], Step [8000/12500],Loss: 0.7554\n",
            "Epoch [30/50], Step [10000/12500],Loss: 0.2607\n",
            "Epoch [30/50], Step [12000/12500],Loss: 0.3730\n",
            "Epoch [31/50], Step [2000/12500],Loss: 0.7725\n",
            "Epoch [31/50], Step [4000/12500],Loss: 1.1933\n",
            "Epoch [31/50], Step [6000/12500],Loss: 1.0277\n",
            "Epoch [31/50], Step [8000/12500],Loss: 0.7743\n",
            "Epoch [31/50], Step [10000/12500],Loss: 0.2443\n",
            "Epoch [31/50], Step [12000/12500],Loss: 0.3484\n",
            "Epoch [32/50], Step [2000/12500],Loss: 0.7043\n",
            "Epoch [32/50], Step [4000/12500],Loss: 1.1685\n",
            "Epoch [32/50], Step [6000/12500],Loss: 0.9359\n",
            "Epoch [32/50], Step [8000/12500],Loss: 0.8165\n",
            "Epoch [32/50], Step [10000/12500],Loss: 0.1876\n",
            "Epoch [32/50], Step [12000/12500],Loss: 0.3187\n",
            "Epoch [33/50], Step [2000/12500],Loss: 0.7213\n",
            "Epoch [33/50], Step [4000/12500],Loss: 1.1372\n",
            "Epoch [33/50], Step [6000/12500],Loss: 0.9060\n",
            "Epoch [33/50], Step [8000/12500],Loss: 0.7716\n",
            "Epoch [33/50], Step [10000/12500],Loss: 0.1703\n",
            "Epoch [33/50], Step [12000/12500],Loss: 0.3115\n",
            "Epoch [34/50], Step [2000/12500],Loss: 0.7233\n",
            "Epoch [34/50], Step [4000/12500],Loss: 1.1568\n",
            "Epoch [34/50], Step [6000/12500],Loss: 0.8599\n",
            "Epoch [34/50], Step [8000/12500],Loss: 0.7657\n",
            "Epoch [34/50], Step [10000/12500],Loss: 0.1204\n",
            "Epoch [34/50], Step [12000/12500],Loss: 0.3052\n",
            "Epoch [35/50], Step [2000/12500],Loss: 0.7755\n",
            "Epoch [35/50], Step [4000/12500],Loss: 1.1984\n",
            "Epoch [35/50], Step [6000/12500],Loss: 0.7637\n",
            "Epoch [35/50], Step [8000/12500],Loss: 0.7965\n",
            "Epoch [35/50], Step [10000/12500],Loss: 0.0917\n",
            "Epoch [35/50], Step [12000/12500],Loss: 0.2858\n",
            "Epoch [36/50], Step [2000/12500],Loss: 0.8126\n",
            "Epoch [36/50], Step [4000/12500],Loss: 1.1436\n",
            "Epoch [36/50], Step [6000/12500],Loss: 0.6558\n",
            "Epoch [36/50], Step [8000/12500],Loss: 0.6706\n",
            "Epoch [36/50], Step [10000/12500],Loss: 0.1009\n",
            "Epoch [36/50], Step [12000/12500],Loss: 0.3080\n",
            "Epoch [37/50], Step [2000/12500],Loss: 0.7484\n",
            "Epoch [37/50], Step [4000/12500],Loss: 1.1245\n",
            "Epoch [37/50], Step [6000/12500],Loss: 0.5801\n",
            "Epoch [37/50], Step [8000/12500],Loss: 0.6854\n",
            "Epoch [37/50], Step [10000/12500],Loss: 0.0702\n",
            "Epoch [37/50], Step [12000/12500],Loss: 0.2881\n",
            "Epoch [38/50], Step [2000/12500],Loss: 0.7928\n",
            "Epoch [38/50], Step [4000/12500],Loss: 1.1469\n",
            "Epoch [38/50], Step [6000/12500],Loss: 0.5211\n",
            "Epoch [38/50], Step [8000/12500],Loss: 0.6643\n",
            "Epoch [38/50], Step [10000/12500],Loss: 0.0714\n",
            "Epoch [38/50], Step [12000/12500],Loss: 0.2363\n",
            "Epoch [39/50], Step [2000/12500],Loss: 0.8481\n",
            "Epoch [39/50], Step [4000/12500],Loss: 1.1398\n",
            "Epoch [39/50], Step [6000/12500],Loss: 0.4920\n",
            "Epoch [39/50], Step [8000/12500],Loss: 0.6713\n",
            "Epoch [39/50], Step [10000/12500],Loss: 0.0583\n",
            "Epoch [39/50], Step [12000/12500],Loss: 0.2177\n",
            "Epoch [40/50], Step [2000/12500],Loss: 0.8199\n",
            "Epoch [40/50], Step [4000/12500],Loss: 1.1448\n",
            "Epoch [40/50], Step [6000/12500],Loss: 0.5379\n",
            "Epoch [40/50], Step [8000/12500],Loss: 0.6726\n",
            "Epoch [40/50], Step [10000/12500],Loss: 0.0570\n",
            "Epoch [40/50], Step [12000/12500],Loss: 0.2570\n",
            "Epoch [41/50], Step [2000/12500],Loss: 0.8088\n",
            "Epoch [41/50], Step [4000/12500],Loss: 1.0920\n",
            "Epoch [41/50], Step [6000/12500],Loss: 0.4612\n",
            "Epoch [41/50], Step [8000/12500],Loss: 0.6709\n",
            "Epoch [41/50], Step [10000/12500],Loss: 0.0395\n",
            "Epoch [41/50], Step [12000/12500],Loss: 0.2315\n",
            "Epoch [42/50], Step [2000/12500],Loss: 0.8491\n",
            "Epoch [42/50], Step [4000/12500],Loss: 1.1252\n",
            "Epoch [42/50], Step [6000/12500],Loss: 0.4601\n",
            "Epoch [42/50], Step [8000/12500],Loss: 0.7089\n",
            "Epoch [42/50], Step [10000/12500],Loss: 0.0348\n",
            "Epoch [42/50], Step [12000/12500],Loss: 0.2382\n",
            "Epoch [43/50], Step [2000/12500],Loss: 0.9022\n",
            "Epoch [43/50], Step [4000/12500],Loss: 1.1081\n",
            "Epoch [43/50], Step [6000/12500],Loss: 0.4302\n",
            "Epoch [43/50], Step [8000/12500],Loss: 0.6556\n",
            "Epoch [43/50], Step [10000/12500],Loss: 0.0295\n",
            "Epoch [43/50], Step [12000/12500],Loss: 0.2758\n",
            "Epoch [44/50], Step [2000/12500],Loss: 0.8181\n",
            "Epoch [44/50], Step [4000/12500],Loss: 1.0155\n",
            "Epoch [44/50], Step [6000/12500],Loss: 0.5068\n",
            "Epoch [44/50], Step [8000/12500],Loss: 0.6848\n",
            "Epoch [44/50], Step [10000/12500],Loss: 0.0348\n",
            "Epoch [44/50], Step [12000/12500],Loss: 0.1652\n",
            "Epoch [45/50], Step [2000/12500],Loss: 0.7878\n",
            "Epoch [45/50], Step [4000/12500],Loss: 1.0344\n",
            "Epoch [45/50], Step [6000/12500],Loss: 0.3700\n",
            "Epoch [45/50], Step [8000/12500],Loss: 0.5882\n",
            "Epoch [45/50], Step [10000/12500],Loss: 0.0377\n",
            "Epoch [45/50], Step [12000/12500],Loss: 0.1670\n",
            "Epoch [46/50], Step [2000/12500],Loss: 0.7591\n",
            "Epoch [46/50], Step [4000/12500],Loss: 0.9981\n",
            "Epoch [46/50], Step [6000/12500],Loss: 0.4480\n",
            "Epoch [46/50], Step [8000/12500],Loss: 0.5544\n",
            "Epoch [46/50], Step [10000/12500],Loss: 0.0465\n",
            "Epoch [46/50], Step [12000/12500],Loss: 0.1531\n",
            "Epoch [47/50], Step [2000/12500],Loss: 0.7089\n",
            "Epoch [47/50], Step [4000/12500],Loss: 1.0681\n",
            "Epoch [47/50], Step [6000/12500],Loss: 0.3281\n",
            "Epoch [47/50], Step [8000/12500],Loss: 0.6232\n",
            "Epoch [47/50], Step [10000/12500],Loss: 0.0514\n",
            "Epoch [47/50], Step [12000/12500],Loss: 0.1839\n",
            "Epoch [48/50], Step [2000/12500],Loss: 0.6287\n",
            "Epoch [48/50], Step [4000/12500],Loss: 0.9616\n",
            "Epoch [48/50], Step [6000/12500],Loss: 0.3527\n",
            "Epoch [48/50], Step [8000/12500],Loss: 0.5306\n",
            "Epoch [48/50], Step [10000/12500],Loss: 0.0521\n",
            "Epoch [48/50], Step [12000/12500],Loss: 0.1981\n",
            "Epoch [49/50], Step [2000/12500],Loss: 0.7013\n",
            "Epoch [49/50], Step [4000/12500],Loss: 0.9290\n",
            "Epoch [49/50], Step [6000/12500],Loss: 0.3398\n",
            "Epoch [49/50], Step [8000/12500],Loss: 0.5915\n",
            "Epoch [49/50], Step [10000/12500],Loss: 0.0408\n",
            "Epoch [49/50], Step [12000/12500],Loss: 0.2420\n",
            "Epoch [50/50], Step [2000/12500],Loss: 0.6740\n",
            "Epoch [50/50], Step [4000/12500],Loss: 0.8149\n",
            "Epoch [50/50], Step [6000/12500],Loss: 0.2943\n"
          ]
        }
      ]
    }
  ]
}